csvSchema {
  id {
    type = "int"
    index = 3
  }
  x {
    type = "double"
    index = 5
  }
  y {
    type = "double"
    index = 6
  }
  epoch {
    type = "long"
    index = 1
  }
  rowSize = 9
}
tiling {
  levels = [0, 1, 2, 3, 4, 5, 6]
  bins = 128
  source = hdfs://user/llay/weird_data
}
xySegment {
  arcType="fullline"
  projection="cartesian"
  minSegLen="100"
  maxSegLen="200"
  x1Column="x1"
  y1Column="y1"
  x2Column="x2"
  x3Column="y2"
  xyBounds=[20, 20, 20, 20]
  zBounds=[1, 7]
  tileSize = 256
}
fileOutput {
  dest="/data/llays/tiling_jobs"
  layer="testCase"
}
spark {
  master=local
}
sparkCsv {
  // Put in any spark csv config one needs here
}
