# pass-through config for spark csv data loader - see
# https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.sql.DataFrameReader under the csv method for full set of args.
sparkCsv {
  sep = "\t"
  header = false
  mode = DROPMALFORMED
}

# maps indexed columns from input CSV to named + typed columns in
# Dataframe
csvSchema {
  rowSize = 5
  timestamp { type = double, index = 0 }
  dest_port { type = long, index = 4 }
}

# spark config - anything in spark block is passed through as a property to the spark context
# see http://spark.apache.org/docs/latest/configuration.html
spark {
  master = "local[*]"
  app.name = salt-port-heatmap-test
}

# general tiling job config
tiling {
  levels = [0,1,2,3,4,5,6]
  bins = 256
  source = ../data/maccdc2012_00008.csv
}

# heatmap config
xyHeatmap {
  projection = cartesian
  xColumn = timestamp
  yColumn = dest_port
  xyBounds = [1331915772,0,1331918927,65535]
}

# file output config
fileOutput {
  dest = ../tiles
  layer = port_heatmap
}
