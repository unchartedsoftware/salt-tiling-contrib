# pass-through config for spark csv data loader - see https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.sql.DataFrameReader under the csv method for full set of args.
# for full set of options
sparkCsv {
  sep = ","
  header = false
  mode = DROPMALFORMED
}

# maps indexed columns from input CSV to named + typed columns in
# Dataframe
csvSchema {
  rowSize = 14
  pickup_lon { type = double, index = 10 }
  pickup_lat { type = double, index = 11 }
  dropoff_lon { type = double, index = 12 }
  dropoff_lat { type = double, index = 13 }
}

# spark config - anything in spark block is passed through as a property to the spark context
# see http://spark.apache.org/docs/latest/configuration.html
spark {
  master = "local[*]"
  app.name = salt-geo-segement-test
}

# general tiling job config
tiling {
  levels = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]
  bins = 256
  source = ../data/taxi_micro.csv
}

xySegment {
  arcType = fullline
  projection = mercator
  minSegLen = 4
  maxSegLen = 1024
  x1Column = pickup_lon
  y1Column = pickup_lat
  x2Column = dropoff_lon
  y2Column = dropoff_lat
}

# file output config
fileOutput {
  dest = ../tiles
  layer = geo_segments
}
