description = "XDATA pipeline operations"
group = "software.uncharted.xdata"
version = "0.1-SNAPSHOT"

// project level shared version variables
ext {
  dependencyScalaVersion = "2.10"
  scalaVersion = "2.10.6"
  sparkVersion = "1.6.2"
  cdhVersion="1.0.0-cdh5.5.2"
  hbaseVersion = "1.0.0-cdh5.5.2"
}

// extra jars for plugins
buildscript {
  repositories {
    mavenCentral()
    jcenter()
    maven {
      url = "https://repository.cloudera.com/artifactory/cloudera-repos"
    }
  }

  dependencies {
    classpath "com.github.maiflai:gradle-scalatest:0.10"
    classpath 'org.scoverage:gradle-scoverage:1.0.9'
    classpath "org.github.ngbinh.scalastyle:gradle-scalastyle-plugin_2.10:0.8.2"
  }
}

apply plugin: 'scala'
apply plugin: 'maven'
apply plugin: 'maven-publish'
apply plugin: 'distribution'
apply plugin: 'idea'
apply plugin: 'scalaStyle'
apply plugin: 'scoverage'
apply plugin: 'com.github.maiflai.scalatest'

// Configure a jar task to build a fat jar, run it before the assembly step
task assemblyJar(type: Jar) {
  classifier = "assembly"
  from files(sourceSets.main.output.classesDir)
  from files(sourceSets.main.output.resourcesDir)
  from {
    (configurations.runtime - configurations.provided).collect {
      it.isDirectory() ? it : zipTree(it)
    }
  }
}
assemblyJar.mustRunAfter "jar"
assemble.dependsOn "assemblyJar"

// run scala style and code coverage checks as part of the check phase
check.dependsOn "scalaStyle"
// temporarily disabled - needs s3 cases to be hit
// check.dependsOn "checkScoverage"


// task to update the gradle wrapper
task wrapper(type: Wrapper) {
  gradleVersion = '2.9'
}

// extra configurations
configurations {
  provided
  compile.extendsFrom provided
  dist
  // IntelliJ is picking up an old version of jetty that we need to
  // force exclude to run tests.
  provided.exclude group: "org.jboss.netty", module: "netty"
}
// Add scripts zip to the artifacts of the dist assembly.  This can be used by downstream builds.
artifacts {
  dist assemblyJar
}

// configure the idea plugin
idea {
  module {
    inheritOutputDirs = false
    outputDir = file("$buildDir/classes/main/")
    scopes.PROVIDED.plus += [ configurations.provided ]
  }
}

// configure scala style checking plugin
scalaStyle {
  configLocation = "scalastyle_config.xml"
  includeTestSourceDirectory = true
  source = sourceSets.main.allScala
  testSource = sourceSets.test.allScala
  failOnWarning = true
  verbose = true
}

// configure scoverage plugin
//checkScoverage {
//  minimumRate = 0.75
//}

// Test and scoverage tests should be forced to single threaded execution
// since Spark can't run multiple contexts within in a single JVM.  We ignore
// tests relying on external systems (HBase + S3) by default, but they can be
// include by adding -PincludeExternalTests to the command line.  Running tests on
// a closed build environment like Bamboo block all IO, so an option is provided
// to block those via -PexcludeIoTests.
test {
  maxParallelForks = 1
  jvmArgs += ["-XX:MaxPermSize=512m", "-Xmx1000m"]
  if (project.hasProperty('excludeIoTests')) {
    tags {
      exclude 's3.test'
      exclude 'hbc.test'
      exclude 'fileio.test'
    }
  } else if (!project.hasProperty('includeExternalTests')) {
    tags {
      exclude 's3.test'
      exclude 'hbc.test'
    }
  }

}

testScoverage {
  maxParallelForks = 1
  jvmArgs += ["-XX:MaxPermSize=512m", "-Xmx1000m"]
  if (project.hasProperty('excludeIoTests')) {
    tags {
      exclude 's3.test'
      exclude 'hbc.test'
      exclude 'fileio.test'
    }
  } else if (!project.hasProperty('includeExternalTests')) {
    tags {
      exclude 's3.test'
      exclude 'hbc.test'
    }
  }
}

// Creation of a distribution archive containing the pipeline assembly jar and the baseline
// execution scripts.
distributions {
  main {
    baseName = 'xdata-pipeline-ops'
    contents {
      // Copy the run scripts over
      into ("scripts"){
        from("src/main/scripts")
        fileMode 0755
      }

      // Copy assembly jar, stripping off the version info.
      into("lib") {
        from configurations.dist.artifacts.files.filter {
          it.name =~ /assembly\w*\.jar/
        }
        rename "(.*)-$version-assembly(.*)", "\$1\$2"
      }
    }
  }
}

// target 1.7 JVM
sourceCompatibility = 1.7
targetCompatibility = 1.7

// repositories for dependencies
repositories {
  mavenCentral()
  mavenLocal()
  maven {
    url = "https://repository.cloudera.com/artifactory/cloudera-repos"
  }
}

// publishing setup - url, username, password need to be stored in environment
// variables
publishing {
  publications {
    maven(MavenPublication) {
      from components.java
    }
  }
}
publishing {
  repositories {
    maven {
      if (version.endsWith("SNAPSHOT")) {
        url "${System.env.MAVEN_REPO_URL}/snapshots"
      } else {
        url "${System.env.MAVEN_REPO_URL}/internal"
      }
      credentials {
        username "${System.env.MAVEN_REPO_USERNAME}"
        password "${System.env.MAVEN_REPO_PASSWORD}"
      }
    }
  }
}

// Common dependency helper functions
ext {
	// A function to exclude various HBase dependencies we don't want.
	hbaseDependency = {dependencyName, config="compile" ->
		println("Dealing with hbase dependency "+dependencyName+" for "+config)

		getDependencies().add(config, "org.apache.hbase:"+dependencyName+":$hbaseVersion", {
			exclude group: "asm", module: "asm"
			exclude group: "org.slf4j", module: "slf4j-api"
			exclude group: "org.slf4j", module: "slf4j-log4j12"
			exclude group: "org.mortbay.jetty"
			exclude group: "org.jboss.netty", module: "netty"
			exclude group: "io.netty", module: "netty"
		})
	}
}

// Jars this project depends on
dependencies {
	// Compile config - needed to build and required at runtime
	compile "software.uncharted.salt:salt-core:3.0.1"
	compile "software.uncharted.sparkpipe:sparkpipe-core:0.9.7"
	compile "org.clapper:grizzled-slf4j_$dependencyScalaVersion:1.0.2"
	compile 'log4j:apache-log4j-extras:1.2.17'
	compile "org.spire-math:spire_$dependencyScalaVersion:0.11.0"
	compile "com.amazonaws:aws-java-sdk:1.3.11"
	compile "com.databricks:spark-csv_$dependencyScalaVersion:1.4.0"
	compile "net.liftweb:lift-json_$dependencyScalaVersion:2.6.2"
  compile "com.typesafe:config:1.3.0"
	compile "org.apache.hbase:hbase-client:$cdhVersion"
	compile "org.apache.hbase:hbase-common:$cdhVersion"
  compile "joda-time:joda-time:2.9.4"
  compile "org.joda:joda-convert:1.8.1"
	hbaseDependency("hbase-client")
	hbaseDependency("hbase-common")
	hbaseDependency("hbase-server")

	// Provided config - needed to compile, expected to be available when deployed
	provided "org.apache.spark:spark-core_$dependencyScalaVersion:$sparkVersion"
	provided "org.apache.spark:spark-sql_$dependencyScalaVersion:$sparkVersion"

	// Test related
	testCompile "org.scalatest:scalatest_$dependencyScalaVersion:3.0.0"
	testRuntime 'org.pegdown:pegdown:1.1.0'
	scoverage "org.scoverage:scalac-scoverage-plugin_$dependencyScalaVersion:1.1.0",
    "org.scoverage:scalac-scoverage-runtime_$dependencyScalaVersion:1.1.0"
}
